# -*- coding: utf-8 -*-
"""final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dPsFX37MUN0C7n1o5-TJrUYdYYSbpYTg

# **Project Title:** Unlocking Customer Behavior for Effective Marketing üìäüîç

**Notebook:** Final Project Notebooküìù
**Team Name**: Data Enthusiasts üë©‚Äçüíªüë®‚Äçüíª  
**Team Members**:  
- **Leader**: Amanullah  
- **Teammate**: Daniyal Hyder   

**Assigned by**: Miss Reema Memon  
**Dataset Name**: Digital Marketing Dataset üìä

\

## GitHub Repository

[Link to GitHub Repository](https://github.com/Amanullahmemon75/final_project_of_PITP) üöÄ

# **Comprehensive Data Preparation and Preprocessing Workflow**

**Installating and importing libraries with their versions**
"""

# Data manipulation libraries
!pip install pandas -q  # For data manipulation and analysis
!pip install numpy -q  # For numerical operations and handling arrays

# Data visualization libraries
!pip install matplotlib -q  # For creating static plots
!pip install seaborn -q  # For statistical data visualization
!pip install plotly -q  # For interactive visualizations

# Machine learning libraries
!pip install scikit-learn -q  # For various machine learning algorithms and tools
!pip install xgboost -q  # For gradient boosting algorithms, including XGBoost

# Hypothesis testing libraries
!pip install statsmodels -q  # For statistical modeling and hypothesis testing
!pip install scipy -q  # For scientific computing and statistics

# Deep learning library
!pip install tensorflow -q  # For deep learning models a

# Data manipulation
import pandas as pd
import numpy as np

# Data visualization
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import plotly
import plotly.express as px

# Machine learning
import sklearn
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from xgboost import XGBClassifier

# Dimensionality reduction
from sklearn.decomposition import PCA

# Hypothesis testing

import scipy
import statsmodels.api as sm
from scipy import stats

# Deep learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Preprocessing and metrics
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Print versions
print("Library Versions:")
print("pandas:", pd.__version__)
print("numpy:", np.__version__)
print("matplotlib:", matplotlib.__version__)
print("seaborn:", sns.__version__)
print("plotly:", plotly.__version__)
print("scikit-learn:", sklearn.__version__)
print("xgboost:", XGBClassifier.__module__.split('.')[0] + ' - Version not directly accessible from classifier')
print("statsmodels:", sm.__version__)
print("scipy:", scipy.__version__)
print("tensorflow:", tf.__version__)

"""**Dataset Loading and Initial Exploration**"""

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/final_project_folder/digital_marketing_campaign_dataset.csv')

# Check the shape of the dataset (rows, columns)
print(f"Shape:\n", df.shape)

#Check the columns
print(f"Column Names:\n", df.columns)

# Preview the first 5 rows of the dataset
print(f"\n Head rows of Dataset:\n",df.head(5))

# Check for basic info like data types and missing values
print("\n Basic Info of Dataset:\n")
print(df.info())

# Summary statistics for numerical columns
print(f"\n Statistical summary:\n",df.describe())

"""---
**Data Wrangling and Cleaning**\
Data Wrangling and Cleaning: The process of transforming and preparing raw data by correcting inaccuracies, removing duplicates, addressing outliers, and dealing with inconsistencies to make it suitable for analysis.

`Data Standardization and Normalization`\
This process typically involves:

1. Case Normalization: Making text lowercase or title case (first letter capitalized) for consistency.
2. Punctuation Removal: Removing unnecessary symbols or punctuation marks.
3. Text Standardization: Ensuring headers and text fields follow a uniform naming convention and format.
"""

# Original column names
# Defining a list of column names with mixed casing for demonstration purposes
columns = [
    'CustomerID', 'Age', 'Gender', 'Income', 'CampaignChannel',
    'CampaignType', 'AdSpend', 'ClickThroughRate', 'ConversionRate',
    'WebsiteVisits', 'PagesPerVisit', 'TimeOnSite', 'SocialShares',
    'EmailOpens', 'EmailClicks', 'PreviousPurchases', 'LoyaltyPoints',
    'AdvertisingPlatform', 'AdvertisingTool', 'Conversion'
]

# Normalize the column names to add underscores and make them more readable
# Convert all column names to lowercase and replace camel case with underscores
df.columns = df.columns.str.replace('([a-z])([A-Z])', r'\1_\2', regex=True).str.lower().str.replace(' ', '_')

# Display the updated column names to verify the changes
print(df.columns)

"""\
**`Handling Missing values`**\
Checking null values by isnull.sum function if exist
"""

#Check the rows and columns of datasets
print("\n Dataset null values:\n", df.isnull().sum())

# Display unique values for each column in the DataFrame
unique_values = df.apply(lambda x: x.unique())

# Print unique values
for column, values in unique_values.items():
    print(f"Unique values in '{column}': {values}")

"""`Droping unnecessary columns`
1. customer_id: This is likely just an identifier and does not provide predictive information regarding conversion.

2. advertising_platform: If it only contains a single unique value ('IsConfid'), it does not provide any useful information for prediction and should be dropped.

3. advertising_tool: Similar to the above, if it contains only one unique value ('ToolConfid'), it does not contribute to the prediction.
"""

# Drop the identified useless columns
df.drop(columns=['customer_id', 'advertising_platform', 'advertising_tool'], inplace=True)
df.head(1)

# Check for duplicate rows
duplicate_rows = df.duplicated()

# Display duplicate rows
print("Duplicate Rows:\n", df[duplicate_rows])

# Count the number of duplicate rows
duplicate_count = duplicate_rows.sum()
print(f"Number of duplicate rows: {duplicate_count}")

"""**`Outliers`**"""

# 2. Identifying Outliers using the IQR method
columns = ['income', 'ad_spend', 'click_through_rate', 'conversion_rate', 'website_visits']

# 2. Identifying Outliers using the IQR method
outliers = {}
for col in columns:
    # Check if the column exists in the DataFrame
    if col in df.columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Find outliers
        outliers[col] = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        print(f"Outliers in '{col}':\n", outliers[col][col])

# Display outliers as a dictionary for easy reference
print("\nOutliers Dictionary:")
for key, value in outliers.items():
    print(f"{key}: {value.shape[0]} outliers found")

# Create separate interactive box plots for each feature
for col in columns:
    if col in df.columns:  # Check if the column exists
        fig = px.box(
            df,
            y=col,
            title=f'Box Plot of {col}',
            points='all',  # Show all points for better visibility
            color_discrete_sequence=px.colors.qualitative.Plotly
        )

        # Update layout for better styling
        fig.update_layout(
            title_font=dict(size=18, color="#333333", family="Arial, sans-serif"),
            yaxis_title='Values',
            yaxis_title_font=dict(size=14, color="#333333"),
            plot_bgcolor='#fafafa',  # Light background for modern look
        )

        # Show the individual plot
        fig.show()

"""---
**Data Transformation**

**`Feature Engineering / Transformation:`**

 Data transformation involves modifying existing data to create new features that can enhance the performance of machine learning models. Feature engineering helps to consolidate information and improve interpretability by combining related metrics into a single representative feature.

`Reason for Combining:`

We combine website_visits and pages_per_visit to create total_page_views as a single indicator of user engagement. By multiplying these two features, we get the total number of pages viewed across all visits, which provides a clearer picture of engagement intensity and may correlate better with conversion likelihood.
"""

# Creating a new feature 'total_page_views'
df['total_page_views'] = df['website_visits'] * df['pages_per_visit']

# Drop the 'website_visits' and 'pages_per_visit' columns
df.drop(['website_visits', 'pages_per_visit'], axis=1, inplace=True)

"""`Reason for Combining:`

The 'email_opens' column has more variety, representing both aspects of engagement, so we combined it with 'email_clicks' into a single column called 'email_activity'. To achieve this unified view of email interaction, we dropped only the 'email_clicks' column and renamed 'email_opens' to 'email_engagement' to represent both variables.
"""

# Drop the 'email_clicks' column
df.drop('email_clicks', axis=1, inplace=True)

# Rename 'email_opens' to 'email_activity'
df.rename(columns={'email_opens': 'email_activity'}, inplace=True)

# Display the updated DataFrame
print(df)

"""**`Feature Encoding:`**

By this method, we encode to categorical variables so machine can understand
"""

from sklearn.preprocessing import LabelEncoder

# Assuming df is your DataFrame
label_encoder = LabelEncoder()
df['gender'] = label_encoder.fit_transform(df['gender'])

# Specify categorical columns that need encoding
categorical_columns = ['campaign_channel', 'campaign_type']

# Apply One-Hot Encoding to the categorical columns directly in the dataframe
df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)

df.head()

"""---
# **Problem 1 and its Answer**

**Problem:** Which customers are at risk of leaving (not making future purchases) based on their engagement data and loyalty points?
\
\
\
Creating a `Flag for At-Risk Customers`: Add a new column into DataFrame that flags customers as "at-risk" based on defined criteria.
"""

# criteria for at-risk customers
def is_at_risk(row):
    return (row['total_page_views'] < threshold_visits) or \
           (row['time_on_site'] < threshold_time) or \
           (row['loyalty_points'] < threshold_loyalty) or \
           (row['previous_purchases'] < threshold_purchases)

threshold_visits = 5
threshold_time = 5  # in minutes
threshold_loyalty = 500
threshold_purchases = 2

df['at_risk'] = df.apply(is_at_risk, axis=1)

# Count the number of at-risk customers
at_risk_count = df['at_risk'].sum()
total_customers = len(df)  # Total number of customers in the DataFrame

# Print the result including the total number of customers
print(f'Number of at-risk customers: {at_risk_count} out of {total_customers} customers.\n')

# Create an interactive count plot using Plotly
import plotly.express as px

fig = px.histogram(
    df,
    x='at_risk',
    color='at_risk',
    color_discrete_sequence=["#4E79A7", "#F28E2B"],  # Soft blue and orange
    title='Distribution of At-Risk Customers',
    labels={'at_risk': 'At-Risk Status'},
    text_auto=True  # Automatically adds text labels above bars
)

# Update layout for better styling
fig.update_layout(
    xaxis_title='At-Risk Status',
    yaxis_title='Customer Count',
    title_font=dict(size=18, color="#333333", family="Arial, sans-serif"),
    xaxis_title_font=dict(size=14, color="#333333"),
    yaxis_title_font=dict(size=14, color="#333333"),
    plot_bgcolor='#fafafa',  # Light background for modern look
    yaxis=dict(showgrid=True, gridcolor='gray', gridwidth=0.7),
)

# Show the plot
fig.show()

# Select only numerical columns by excluding categorical ones
numerical_df = df.select_dtypes(include=[float, int])

# Calculate correlation matrix for numerical columns
correlation_matrix = numerical_df.corr()

# Plotting the correlation matrix with a wider layout
plt.figure(figsize=(18, 15))  # Wider width for better view
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""---
# **Problem 2 and its Answer**

**Problem:** How can we segment customers based on engagement behavior to develop targeted marketing strategies for each group?
"""

# Step 1: Select features related to engagement for clustering
engagement_features = df[['total_page_views', 'click_through_rate', 'conversion_rate', 'time_on_site',
                          'social_shares', 'email_activity', 'previous_purchases', 'loyalty_points']]

# Step 2: Standardize the features for better clustering performance
scaler = StandardScaler()
scaled_features = scaler.fit_transform(engagement_features)

# Step 3: Apply KMeans clustering
# We‚Äôll start with an arbitrary number of clusters, e.g., 3 (low, medium, and high engagement)
kmeans = KMeans(n_clusters=3, random_state=42)
df['engagement_segment'] = kmeans.fit_predict(scaled_features)

# Step 4: Analyze and interpret the clusters
# Calculate the average values for each cluster to understand engagement levels
cluster_summary = df.groupby('engagement_segment')[engagement_features.columns].mean()
print(cluster_summary)

# Create an interactive count plot
fig = px.histogram(df,
                   x='engagement_segment',
                   color='engagement_segment',
                   title='Distribution of Engagement Segments',
                   labels={'engagement_segment': 'Engagement Segment'},
                   color_discrete_sequence=px.colors.qualitative.Plotly)

# Update layout for better styling
fig.update_layout(
    xaxis_title='Engagement Segment',
    yaxis_title='Number of Customers',
    title_font=dict(size=18, color="#333333", family="Arial, sans-serif"),
    xaxis_title_font=dict(size=14, color="#333333"),
    yaxis_title_font=dict(size=14, color="#333333"),
    plot_bgcolor='#fafafa',  # Light background for modern look
)

# Show the plot
fig.show()

"""**Insights by Segments:**


The engagement metrics across segments suggest tailored strategies to boost purchases. Segment 0 has high page views but low click-through and conversion rates, so enhancing content marketing and SEO could encourage more engagement. Segment 1 shows the highest click-through rate, indicating responsiveness to marketing efforts; targeted campaigns with strong Calls-to-Action (CTAs) may drive conversions here. Segment 2, with the highest conversion rate but lower page views, would benefit from personalized outreach and incentives to capitalize on purchasing readiness. These strategies can enhance segment-specific engagement and overall sales.

---
# **Problem 3 and its Answer**

**Problem statement:** Conduct hypothesis testing to evaluate the effectiveness of Social Media and PPC (Pay-Per-Click) marketing campaigns. The goal is to determine if the conversion rates of both channels are equal, assessing whether they drive conversions equally effectively.

**Hypothesis Statement**

- **Null Hypothesis (H‚ÇÄ)**: The conversion rates of Social Media and PPC (Pay-Per-Click) campaigns are equal, indicating that both channels drive conversions with equal effectiveness.
- **Alternative Hypothesis (H‚ÇÅ)**: The conversion rates of Social Media and PPC (Pay-Per-Click) campaigns are not equal.
"""

import pandas as pd
from scipy import stats

# Step 1: Filter conversion rate data for Social Media and PPC campaigns
social_media_conversions = df[df['campaign_channel_Social Media'] == True]['conversion_rate']
ppc_conversions = df[df['campaign_channel_PPC'] == True]['conversion_rate']

# Step 2: Calculate mean and standard deviation for each channel
mean_social = social_media_conversions.mean()
mean_ppc = ppc_conversions.mean()
std_social = social_media_conversions.std()
std_ppc = ppc_conversions.std()

# Step 3: Conduct an independent t-test to compare conversion rates
t_stat, p_value = stats.ttest_ind(social_media_conversions, ppc_conversions, equal_var=False)

# Step 4: Display results
print(f"Mean Conversion Rate (Social Media): {mean_social:.4f}")
print(f"Mean Conversion Rate (PPC): {mean_ppc:.4f}")
print(f"T-Statistic: {t_stat:.4f}")
print(f"P-Value: {p_value:.4f}")

# Step 5: Interpret results based on a 5% significance level
alpha = 0.05
if p_value < alpha:
    print("Reject the null hypothesis: There is a significant difference in conversion rates between Social Media and PPC (Pay-Per-Click) campaigns.")
else:
    print("Fail to reject the null hypothesis: No significant difference in conversion rates between Social Media and PPC (Pay-Per-Click) campaigns.")

"""---

# **Problem 4 and its Answer**

**Problem:** How effective are different marketing channels and campaigns in driving customer conversion and engagement, as measured by CTR and conversion rate?
"""

# Step 1: Map binary columns to channel and type categories for grouping
df['campaign_channel'] = df[['campaign_channel_PPC', 'campaign_channel_Referral', 'campaign_channel_SEO', 'campaign_channel_Social Media']].idxmax(axis=1)
df['campaign_type'] = df[['campaign_type_Consideration', 'campaign_type_Conversion', 'campaign_type_Retention']].idxmax(axis=1)

# Step 2: Group data by 'campaign_channel' and 'campaign_type' and calculate average CTR and conversion rate
channel_campaign_summary = df.groupby(['campaign_channel', 'campaign_type']).agg(
    avg_ctr=('click_through_rate', 'mean'),
    avg_conversion_rate=('conversion_rate', 'mean')
).reset_index()

# Step 3: Reshape the data for plotting (melt the dataframe)
channel_campaign_summary_melted = channel_campaign_summary.melt(
    id_vars=['campaign_channel', 'campaign_type'],
    value_vars=['avg_ctr', 'avg_conversion_rate'],
    var_name='Metric',
    value_name='Value'
)

# Step 4: Create interactive grouped bar plot for both CTR and Conversion Rate
fig_combined = px.bar(
    channel_campaign_summary_melted,
    x='campaign_channel',
    y='Value',
    color='Metric',
    barmode='group',
    facet_col='campaign_type',
    title='Average CTR and Conversion Rate by Campaign Channel and Type',
    labels={'Value': 'Rate'},
)

# Show the combined plot
fig_combined.show()

"""---
# **Main Problem and it Answer by Executing an algorithm:**

**Problem:** How can we use classification models to predict customer conversion based on factors like demographics, campaign performance, and engagement metrics?

## **`Features Selection`**
"""

# Drop 'campaign_channel' and 'campaign_type' columns, that was regenerated to see effective campaign type and channel
df = df.drop(columns=['campaign_channel', 'campaign_type'])

# Also drop other unnecessary variables, was created to answer above problems
df = df.drop(columns=['at_risk', 'engagement_segment'])
#Check the columns
print(f"Column Names:\n", df.columns)

# Separate features and target
X = df.drop(columns=['conversion'])
y = df['conversion']

# Initialize and fit the Random Forest model
model = RandomForestClassifier(random_state=42)
model.fit(X, y)

# Extract feature importances
feature_importances = model.feature_importances_

# Create a DataFrame for the feature importances and sort by importance
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Print the sorted feature importances
print("Top Features by Importance:")
print(importance_df)

# Plot the feature importances using a bar plot
plt.figure(figsize=(12, 8))
sns.barplot(data=importance_df, x='Importance', y='Feature', palette='viridis')
plt.title("Feature Importances by RandomForestClassifier")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# Initialize and fit the Gradient Boosting model
model = GradientBoostingClassifier(random_state=42)
model.fit(X, y)

# Extract feature importances
feature_importances = model.feature_importances_

# Create DataFrame and sort
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(12, 8))
sns.barplot(data=importance_df, x='Importance', y='Feature', palette='viridis')
plt.title("Feature Importances by GradientBoostingClassifier")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

"""After applying two different approaches of verifying important features we selected this features to put in X and dropping other column from X:
- total_page_views    
- time_on_site    
- click_through_rate    
- ad_spend  
- conversion_rate
- loyalty_points   
- email_activity  
- income    
- previous_purchases    
- social_shares    
- age
"""

# Dropping columns with low feature importance
columns_to_drop = [
    'campaign_type_Conversion',
    'campaign_type_Consideration',
    'campaign_type_Retention',
    'campaign_channel_SEO',
    'campaign_channel_Social Media',
    'campaign_channel_Referral',
    'campaign_channel_PPC'
]

# Dropping the columns from the dataset
df = df.drop(columns=columns_to_drop)
df.head()

"""## Machine Learning (ML)"""

import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier

# Define features and target
X = df.drop(columns=['conversion'])
y = df['conversion']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the models
models = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM (RBF Kernel)': SVC(kernel='rbf', random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Neural Network (MLP)': MLPClassifier(max_iter=1000, random_state=42)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Fit the model
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Calculate accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Print initial output
    print("Predictions:", y_pred[:20])  # Show first 20 predictions
    print("Actual Values:", y_test.values[:20])  # Show first 20 actual values
    print(f"\n{model_name} Accuracy: {accuracy*100:.2f}%")  # Show accuracy as percentage

    # Print confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"Confusion Matrix:\n{cm}\n")

    # Create classification report in a DataFrame format and include overall metrics
    report = classification_report(y_test, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose()

    # Print classification report as a table including all metrics
    print(f"Classification Report ({model_name}):")
    display(report_df)  # display() for Jupyter; use print(report_df) otherwise
    print("\n" + "-"*50 + "\n")

"""## **Applying Oversampling Technique**

For models where the performance on the minority class (0) is significantly poor, oversampling was applied selectively. This decision was based on the classification reports and confusion matrices of different models, indicating an imbalance issue. For instance:

- **Support Vector Machine (SVM)**: The model performed very well on class 1 (recall = 1.00) but failed completely on class 0 (precision = 0.00, recall = 0.00). This imbalance resulted in a high weighted average but a poor macro average, highlighting the performance gap.
- **K-Nearest Neighbors (KNN)**: Achieved reasonable accuracy on class 1 but struggled with class 0 (recall = 0.03), resulting in an imbalanced prediction focus.
- **Neural Network (MLP)**: Similarly to SVM, the model predicted only class 1, completely missing class 0 instances.

To address this imbalance and improve the model's ability to learn patterns in class 0, oversampling techniques were used to augment class 0 samples and provide the model with a more balanced training dataset. This approach aims to improve the model's recall and precision for class 0, enhancing overall performance across both classes.
"""

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

X = df.drop(columns=['conversion'])
y = df['conversion']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE (oversampling the minority class) or RandomUnderSampler (undersampling the majority class)
# Uncomment one of the following depending on which resampling technique you'd like to use

# For Oversampling using SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# For Undersampling using RandomUnderSampler
#undersampler = RandomUnderSampler(random_state=42)
#X_train_res, y_train_res = undersampler.fit_resample(X_train, y_train)

# Initialize the models
models = {
    'SVM (RBF Kernel)': SVC(kernel='rbf', random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Neural Network (MLP)': MLPClassifier(max_iter=1000, random_state=42)
}

# Train and evaluate each model
for model_name, model in models.items():
    # Fit the model
    model.fit(X_train_res, y_train_res)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Calculate accuracy, precision, recall, F1 score
    accuracy = accuracy_score(y_test, y_pred) * 100  # In percentage
    precision = precision_score(y_test, y_pred, average='binary')
    recall = recall_score(y_test, y_pred, average='binary')
    f1 = f1_score(y_test, y_pred, average='binary')

    # Display the first few predictions and actual values
    print("Predictions:", y_pred[:15])  # Show first 5 predictions
    print("Actual Values:", y_test.values[:15])  # Show first 5 actual values

    # Display model performance
    print(f"\n{model_name} Accuracy: {accuracy:.2f}%")  # Show accuracy as percentage

    # Print confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print(f"Confusion Matrix:\n{cm}\n")

    # Create classification report in a DataFrame format and include overall metrics
    report = classification_report(y_test, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose()

    # Print classification report as a table including all metrics
    print(f"Classification Report ({model_name}):")
    display(report_df)  # display() for Jupyter; use print(report_df) otherwise
    print("\n" + "-"*50 + "\n")

"""## **Hyperparameter Tuning**

**Applying Hyperparameter Tuning on models:**

1. Decission Tree Accuracy: 81.06%
2. Random Forest Performance
Accuracy: 89.31%
3. Gradient Boosting Performance
Accuracy: 89.94%\
\
These models' accuracy is taken from the after oversampling results


4. SVM (RBF Kernel) Accuracy: 54.69%
5. K-Nearest Neighbors Accuracy: 62.88%
6. Neural Network (MLP) Accuracy: 17.81%

### Decision Tree Hyperparameter Tuning (with Accuracy comparison)
"""

# Before Hyperparameter Tuning Accuracy
dt_accuracy_before = 81.06  # Set the manually provided accuracy value

print("Decision Tree Accuracy before tuning: {:.2f}%".format(dt_accuracy_before))

# Define the parameter grid for Decision Tree
param_grid_dt = {
    'max_depth': [3, 5, 10, None],  # Max depth of the tree
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node
    'max_features': ['auto', 'sqrt', 'log2', None]  # Number of features to consider for the best split
}

# Initialize the GridSearchCV for Decision Tree
grid_dt = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=5)

# Train the model using Grid Search
grid_dt.fit(X_train, y_train)

# Best parameters from Grid Search
print("Best parameters for Decision Tree:", grid_dt.best_params_)

# Evaluate the model with the best parameters
best_dt = grid_dt.best_estimator_
y_pred_after = best_dt.predict(X_test)

# Accuracy after tuning
dt_accuracy_after = accuracy_score(y_test, y_pred_after) * 100
print("\nDecision Tree Accuracy after tuning: {:.2f}%".format(dt_accuracy_after))

# Calculate the difference in accuracy
accuracy_diff = dt_accuracy_after - dt_accuracy_before
print("\nDifference in Accuracy (after - before): {:.2f}%".format(accuracy_diff))

"""### Gradient Boosting Hyperparameter Tuning (with Accuracy comparison)"""

# Before Hyperparameter Tuning Accuracy
gb_accuracy_before = 89.94  # Set the manually provided accuracy value

print("Gradient Boosting Accuracy before tuning: {:.2f}%".format(gb_accuracy_before))

# Define a smaller parameter grid for GridSearchCV
param_grid_gb = {
    'n_estimators': [50, 100],  # Number of boosting stages (reduced to 2 options)
    'learning_rate': [0.1, 0.2],  # Step size for each boosting step (reduced to 2 options)
    'max_depth': [3, 5],  # Maximum depth of the tree (reduced to 2 options)
    'subsample': [0.8, 1.0],  # Fraction of samples used for fitting each base learner (reduced to 2 options)
    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node (reduced to 2 options)
    'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node (reduced to 2 options)
}

# Initialize the GridSearchCV for Gradient Boosting with fewer folds and parallelization
grid_gb = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    param_grid_gb,
    cv=3,  # Use fewer folds (reduced from 5 to 3)
    n_jobs=-1,  # Use all cores to parallelize
    verbose=1  # Verbose output to track progress
)

# Train the model using Grid Search
grid_gb.fit(X_train, y_train)

# Best parameters from Grid Search
print("Best parameters for Gradient Boosting:", grid_gb.best_params_)

# Evaluate the model with the best parameters
best_gb = grid_gb.best_estimator_
y_pred_after_gb = best_gb.predict(X_test)

# Accuracy after tuning
gb_accuracy_after = accuracy_score(y_test, y_pred_after_gb) * 100
print("\nGradient Boosting Accuracy after tuning: {:.2f}%".format(gb_accuracy_after))

# Calculate the difference in accuracy
accuracy_diff_gb = gb_accuracy_after - gb_accuracy_before
print("\nDifference in Accuracy (after - before): {:.2f}%".format(accuracy_diff_gb))

"""### Random Forest Hyperparameter Tuning (with Accuracy comparison)"""

# Before Hyperparameter Tuning Accuracy
rf_accuracy_before = 89.31  # Set the manually provided accuracy value

print("Random Forest Accuracy before tuning: {:.2f}%".format(rf_accuracy_before))

# Define the parameter grid for Random Forest with fewer options for faster tuning
param_grid_rf = {
    'n_estimators': [50, 100],  # Number of trees in the forest (reduced)
    'max_depth': [None, 10],  # Max depth of the tree (reduced options)
    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node
    'max_features': ['auto', 'sqrt'],  # Number of features to consider for the best split
    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees
}

# Initialize the GridSearchCV for Random Forest with fewer folds and reduced parameter grid
grid_rf = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid_rf,
    cv=3,  # Reduced the number of folds (faster)
    n_jobs=-1,  # Use all cores to parallelize
    verbose=1  # Added verbosity to track the progress
)

# Train the model using Grid Search
grid_rf.fit(X_train, y_train)

# Best parameters from Grid Search
print("Best parameters for Random Forest:", grid_rf.best_params_)

# Evaluate the model with the best parameters
best_rf = grid_rf.best_estimator_
y_pred_after_rf = best_rf.predict(X_test)

# Accuracy after tuning
rf_accuracy_after = accuracy_score(y_test, y_pred_after_rf) * 100
print("\nRandom Forest Accuracy after tuning: {:.2f}%".format(rf_accuracy_after))

# Calculate the difference in accuracy
accuracy_diff_rf = rf_accuracy_after - rf_accuracy_before
print("\nDifference in Accuracy (after - before): {:.2f}%".format(accuracy_diff_rf))

"""### SVM (RBF Kernel) Hyperparameter Tuning (with Accuracy comparison)"""

# Before Hyperparameter Tuning Accuracy
svm_accuracy_before = 54.69  # Set the manually provided accuracy value

print("SVM (RBF Kernel) Accuracy before tuning: {:.2f}%".format(svm_accuracy_before))

# Define the parameter grid for SVM (RBF Kernel)
param_grid_svm = {
    'C': [0.01, 0.1, 1, 10, 100],  # Regularization parameter
    'gamma': ['scale', 'auto', 0.001, 0.01],  # Kernel coefficient
    'kernel': ['rbf'],  # Using RBF kernel
}

# Initialize the GridSearchCV for SVM (RBF Kernel)
grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)

# Train the model using Grid Search
grid_svm.fit(X_train, y_train)

# Best parameters from Grid Search
print("Best parameters for SVM (RBF Kernel):", grid_svm.best_params_)

# Evaluate the model with the best parameters
best_svm = grid_svm.best_estimator_
y_pred_after_svm = best_svm.predict(X_test)

# Accuracy after tuning
svm_accuracy_after = accuracy_score(y_test, y_pred_after_svm) * 100
print("\nSVM (RBF Kernel) Accuracy after tuning: {:.2f}%".format(svm_accuracy_after))

# Calculate the difference in accuracy
accuracy_diff_svm = svm_accuracy_after - svm_accuracy_before
print("\nDifference in Accuracy (after - before): {:.2f}%".format(accuracy_diff_svm))

"""### K-Nearest Neighbors Hyperparameter Tuning (with Accuracy comparison)"""

# Before Hyperparameter Tuning Accuracy
knn_accuracy_before = 62.88  # Set the manually provided accuracy value

print("K-Nearest Neighbors Accuracy before tuning: {:.2f}%".format(knn_accuracy_before))

# Define the parameter grid for K-Nearest Neighbors
param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Number of neighbors to use
    'weights': ['uniform', 'distance'],  # Weight function for prediction
    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metric
}

# Initialize the GridSearchCV for K-Nearest Neighbors
grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5)

# Train the model using Grid Search
grid_knn.fit(X_train, y_train)

# Best parameters from Grid Search
print("Best parameters for K-Nearest Neighbors:", grid_knn.best_params_)

# Evaluate the model with the best parameters
best_knn = grid_knn.best_estimator_
y_pred_after_knn = best_knn.predict(X_test)

# Accuracy after tuning
knn_accuracy_after = accuracy_score(y_test, y_pred_after_knn) * 100
print("\nK-Nearest Neighbors Accuracy after tuning: {:.2f}%".format(knn_accuracy_after))

# Calculate the difference in accuracy
accuracy_diff_knn = knn_accuracy_after - knn_accuracy_before
print("\nDifference in Accuracy (after - before): {:.2f}%".format(accuracy_diff_knn))

"""### Neural Network (MLP) Hyperparameter Tuning (with Accuracy comparison)"""

# Before Hyperparameter Tuning Accuracy
mlp_accuracy_before = 17.81  # Set the manually provided accuracy value

print("Neural Network (MLP) Accuracy before tuning: {:.2f}%".format(mlp_accuracy_before))

# Define a smaller parameter grid for Neural Network (MLP)
param_grid_mlp = {
    'hidden_layer_sizes': [(50,), (100,)],  # Reduced number of neurons in the hidden layers
    'activation': ['relu', 'tanh'],  # Reduced activation functions
    'solver': ['adam', 'sgd'],  # Reduced optimization algorithms
    'alpha': [0.0001, 0.001],  # Reduced regularization strength
    'learning_rate': ['constant', 'adaptive'],  # Reduced learning rate schedules
}

# Initialize the GridSearchCV for Neural Network (MLP) with fewer folds and parallelization
grid_mlp = GridSearchCV(
    MLPClassifier(max_iter=1000, random_state=42),
    param_grid_mlp,
    cv=3,  # Use fewer folds (reduced from 5 to 3)
    n_jobs=-1,  # Use all cores to parallelize the grid search
    verbose=1  # Verbose output to track progress
)

# Train the model using Grid Search
grid_mlp.fit(X_train, y_train)

# Best parameters from Grid Search
print("Best parameters for Neural Network (MLP):", grid_mlp.best_params_)

# Evaluate the model with the best parameters
best_mlp = grid_mlp.best_estimator_
y_pred_after_mlp = best_mlp.predict(X_test)

# Accuracy after tuning
mlp_accuracy_after = accuracy_score(y_test, y_pred_after_mlp) * 100
print("\nNeural Network (MLP) Accuracy after tuning: {:.2f}%".format(mlp_accuracy_after))

# Calculate the difference in accuracy
accuracy_diff_mlp = mlp_accuracy_after - mlp_accuracy_before
print("\nDifference in Accuracy (after - before): {:.2f}%".format(accuracy_diff_mlp))